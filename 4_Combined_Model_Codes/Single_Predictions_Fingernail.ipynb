{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def regression_model_setup():\n",
        "  class EfficientNet9ChannelsModel(nn.Module):\n",
        "        def __init__(self, pretrained=True):\n",
        "            super(EfficientNet9ChannelsModel, self).__init__()\n",
        "            # Load the pre-trained EfficientNet model\n",
        "            self.efficientnet = models.efficientnet_b0(pretrained=pretrained)\n",
        "\n",
        "            # Modify the first convolution layer to accept 3 input channels\n",
        "            self.efficientnet.features[0][0] = nn.Conv2d(\n",
        "                3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
        "            )\n",
        "\n",
        "            # for param in self.efficientnet.parameters():\n",
        "            #     param.requires_grad = False\n",
        "\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "            # Modify the classifier to output 1 value (for regression)\n",
        "            self.efficientnet.classifier = nn.Sequential(\n",
        "                nn.Linear(self.efficientnet.classifier[1].in_features, 1)\n",
        "            )\n",
        "\n",
        "            # for param in self.efficientnet.classifier.parameters():\n",
        "            #     param.requires_grad = True\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.efficientnet.features(x)\n",
        "            x = self.efficientnet.avgpool(x)  # Use the pre-defined avgpool\n",
        "            x = torch.flatten(x, 1)\n",
        "            x = self.dropout(x)\n",
        "            x = self.efficientnet.classifier(x)\n",
        "            return x\n",
        "\n",
        "  model = EfficientNet9ChannelsModel(pretrained=True) #MobileNet9ChannelsModel(pretrained=True)\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "  return model, criterion, optimizer#, scheduler"
      ],
      "metadata": {
        "id": "7t1X_4OwH0PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(save_path=\"regression_model.pth\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, _, _ = regression_model_setup()  # Initialize the model structure\n",
        "    model.load_state_dict(torch.load(save_path, map_location=device))  # Load the state dictionary\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    print(f\"Model loaded from {save_path}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "jFCiBrRrHsZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def created_model_input(results):\n",
        "  nail_class = 0\n",
        "  blue_circle_class_id = 1\n",
        "\n",
        "  top_nail_bounds = {}\n",
        "  normalized = {}\n",
        "  normalized_padded = {}\n",
        "\n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Resize((224, 224))\n",
        "    ])\n",
        "\n",
        "  for result in results:\n",
        "    image_name = result.path.split('/')[-1]\n",
        "    boxes = result.boxes.xyxy.cpu().numpy()  # Bounding boxes (x_min, y_min, x_max, y_max)\n",
        "    scores = result.boxes.conf.cpu().numpy()  # Confidence scores\n",
        "    classes = result.boxes.cls.cpu().numpy()  # Class IDs\n",
        "\n",
        "    nail_boxes = [(box, score) for box, score, cls in zip(boxes, scores, classes) if cls == nail_class]\n",
        "    nail_boxes = sorted(nail_boxes, key=lambda x: x[1], reverse=True)[:3]\n",
        "    top_nail_bounds[image_name] = nail_boxes\n",
        "\n",
        "    normalized_images = []\n",
        "    image_path = result.path\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    blue_circle_box = next(\n",
        "          (box for box, cls in zip(result.boxes.xyxy.cpu().numpy(), result.boxes.cls.cpu().numpy())\n",
        "          if cls == blue_circle_class_id),\n",
        "          None\n",
        "          )\n",
        "    # if blue_circle_box is None:\n",
        "    #   print(image_name)\n",
        "    #   continue\n",
        "\n",
        "      # Compute the center pigment value\n",
        "    x_min, y_min, x_max, y_max = map(int, blue_circle_box)\n",
        "    center_x = (x_min + x_max) // 2\n",
        "    center_y = (y_min + y_max) // 2\n",
        "    center_pixel_value = image[center_y, center_x]  # BGR pixel value\n",
        "\n",
        "    for (box, score) in top_nail_bounds.get(image_name, []):\n",
        "      x_min, y_min, x_max, y_max = map(int, box)\n",
        "      nail_region = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        # Normalize pixel values by center pigment\n",
        "      normalized_nail = nail_region / center_pixel_value\n",
        "      normalized_images.append(nail_region)#(normalized_nail)\n",
        "\n",
        "    normalized[image_name] = normalized_images\n",
        "\n",
        "    for key, images in normalized.items():\n",
        "      while len(images) < 3:\n",
        "          images.append(images[-1])  # Duplicate the last image until there are 3\n",
        "      normalized_padded[key] = images[:3]\n",
        "\n",
        "    combined_image = torch.cat([transform(img) for img in images], dim=2)\n",
        "\n",
        "  return combined_image"
      ],
      "metadata": {
        "id": "dnoV5D-2pdQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path, seg_model_path):\n",
        "  model = YOLO(seg_model_path)\n",
        "  result = model(image_path, save=True)\n",
        "  combined_img = created_model_input(result)\n",
        "  return combined_img"
      ],
      "metadata": {
        "id": "25SKxMedIEqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85VG2zqhHhTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fcb801-8be9-44d5-9c24-003f4e21d072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.39)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/AI_ML_Project/Right_Fingernail/test_1/1709970910465.jpg: 1024x1024 4 nails, 1 colorcard, 1 bluecircle, 566.4ms\n",
            "Speed: 12.8ms preprocess, 566.4ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
            "Results saved to \u001b[1mruns/detect/predict14\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "from google.colab import drive\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "image_path = \"/content/drive/MyDrive/AI_ML_Project/Right_Fingernail/test_1/1709970910465.jpg\"  # Replace with the path to your new image\n",
        "seg_model_path = '/content/drive/MyDrive/AI_ML_Project/Right_Fingernail/best.pt'\n",
        "input_tensor = preprocess(image_path, seg_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_model = load_model('/content/drive/MyDrive/AI_ML_Project/BEST/regression_model.pth')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_tensor = input_tensor.unsqueeze(0)\n",
        "input_tensor = input_tensor.to(device)\n",
        "with torch.no_grad():\n",
        "    prediction = reg_model(input_tensor)\n",
        "\n",
        "print(f\"Predicted Hemoglobin (Hb) value: {prediction.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP7sJJ2R4dt5",
        "outputId": "50da423f-f258-4792-d94a-0247a44e4d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/drive/MyDrive/AI_ML_Project/BEST/regression_model.pth\n",
            "Predicted Hemoglobin (Hb) value: 11.9568\n"
          ]
        }
      ]
    }
  ]
}