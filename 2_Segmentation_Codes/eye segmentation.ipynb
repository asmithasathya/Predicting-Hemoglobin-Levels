{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaVMQ5WMZBhFG0zdxJs8KC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"v6wdpk0E3KxN"},"outputs":[],"source":["# eye segmentation using YOLOv8\n","import sys\n","import subprocess\n","\n","# function to install a package\n","def install(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# install necessary packages if not already installed\n","def install_packages():\n","    packages = ['ultralytics', 'opencv-python-headless', 'Pillow', 'scikit-learn', 'numpy']\n","    for package in packages:\n","        try:\n","            __import__(package.split('-')[0])  # import the base package name\n","        except ImportError:\n","            print(f\"Installing {package}...\")\n","            install(package)\n","\n","install_packages()"]},{"cell_type":"code","source":["import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import numpy as np\n","import cv2\n","from ultralytics import YOLO\n","from albumentations import Compose, HorizontalFlip, RandomCrop, Resize"],"metadata":{"id":"CJzsWQUt3hOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define paths\n","images_path = '/content/drive/MyDrive/dataset/images/Images_left_eye'\n","masks_path = '/content/drive/MyDrive/dataset/masks/left_eye_segmask/SegmentationClass'\n","\n","# paths for train and validation sets\n","train_images_path = '/content/drive/MyDrive/dataset/train/images'\n","train_masks_path = '/content/drive/MyDrive/dataset/train/mask'\n","val_images_path = '/content/drive/MyDrive/dataset/val/images'\n","val_masks_path = '/content/drive/MyDrive/dataset/val/mask'\n","\n","# create directories if they don't exist\n","os.makedirs(train_images_path, exist_ok=True)\n","os.makedirs(train_masks_path, exist_ok=True)\n","os.makedirs(val_images_path, exist_ok=True)\n","os.makedirs(val_masks_path, exist_ok=True)"],"metadata":{"id":"T6U4fatY3ju9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list all image files and filter those with corresponding masks\n","image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","images_with_masks = []\n","for file in image_files:\n","    filename_base = os.path.splitext(file)[0]\n","    mask_file = os.path.join(masks_path, filename_base + '.png')  # assuming masks are .png files\n","    if os.path.exists(mask_file):\n","        images_with_masks.append(file)\n","    else:\n","        print(f\"No mask found for {file}; excluding from dataset.\")"],"metadata":{"id":"qo18YYL63oW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resize images and masks to a fixed size and apply augmentations\n","def preprocess_image_and_mask(image_path, mask_path, output_image_path, output_mask_path):\n","    # load image and mask\n","    image = cv2.imread(image_path)\n","    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","    if image is None or mask is None:\n","        print(f\"Failed to load {image_path} or {mask_path}. Skipping.\")\n","        return\n","\n","    # resize both image and mask to 640x640\n","    resize_transform = Resize(height=640, width=640, interpolation=cv2.INTER_NEAREST)\n","    augmented = resize_transform(image=image, mask=mask)\n","    resized_image = augmented['image']\n","    resized_mask = augmented['mask']\n","\n","    # save resized image and mask\n","    cv2.imwrite(output_image_path, resized_image)\n","    cv2.imwrite(output_mask_path, resized_mask)\n","\n","def augment_data(image_path, mask_path, output_image_path, output_mask_path):\n","    # define augmentations\n","    augmentations = Compose([\n","        HorizontalFlip(p=0.5),\n","        RandomCrop(height=512, width=512, p=1.0)\n","    ])\n","\n","    # load image and mask\n","    image = cv2.imread(image_path)\n","    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","    if image is None or mask is None:\n","        print(f\"Failed to load {image_path} or {mask_path}. Skipping.\")\n","        return\n","\n","    # apply augmentations\n","    augmented = augmentations(image=image, mask=mask)\n","    augmented_image = augmented['image']\n","    augmented_mask = augmented['mask']\n","\n","    # save augmented data\n","    cv2.imwrite(output_image_path, augmented_image)\n","    cv2.imwrite(output_mask_path, augmented_mask)"],"metadata":{"id":"ieN7Vum53qp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess and copy images and masks to train and val directories\n","for file in images_with_masks:\n","    filename_base = os.path.splitext(file)[0]\n","    image_path = os.path.join(images_path, file)\n","    mask_path = os.path.join(masks_path, filename_base + '.png')\n","\n","    # split into train and val sets\n","train_files, val_files = train_test_split(images_with_masks, test_size=0.2, random_state=42)\n","\n","# preprocess and copy images and masks to train and val directories\n","for file in images_with_masks:\n","    filename_base = os.path.splitext(file)[0]\n","    image_path = os.path.join(images_path, file)\n","    mask_path = os.path.join(masks_path, filename_base + '.png')\n","\n","    if file in train_files:\n","        preprocess_image_and_mask(image_path, mask_path,\n","                                  os.path.join(train_images_path, file),\n","                                  os.path.join(train_masks_path, filename_base + '.png'))\n","    else:\n","        preprocess_image_and_mask(image_path, mask_path,\n","                                  os.path.join(val_images_path, file),\n","                                  os.path.join(val_masks_path, filename_base + '.png'))"],"metadata":{"id":"a6XrCYnW3u3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert masks to YOLO segmentation labels\n","def convert_masks_to_yolo_format(images_dir, masks_dir, labels_dir):\n","    os.makedirs(labels_dir, exist_ok=True)\n","    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","    for image_file in image_files:\n","        image_path = os.path.join(images_dir, image_file)\n","        mask_path = os.path.join(masks_dir, os.path.splitext(image_file)[0] + '.png')\n","        label_path = os.path.join(labels_dir, os.path.splitext(image_file)[0] + '.txt')\n","\n","        if not os.path.exists(mask_path):\n","            print(f\"Mask not found for {image_file}, skipping.\")\n","            continue\n","\n","        # read image to get dimensions\n","        img = cv2.imread(image_path)\n","        if img is None:\n","            print(f\"Failed to read image {image_file}, skipping.\")\n","            continue\n","        height, width = img.shape[:2]\n","\n","        # read and process mask\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        if mask is None:\n","            print(f\"Failed to read mask {mask_path}, skipping.\")\n","            continue\n","\n","        # ensure binary mask\n","        mask = np.where(mask > 0, 1, 0).astype(np.uint8)\n","\n","        # find contours\n","        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        if len(contours) == 0:\n","            print(f\"No contours found for {image_file}, skipping.\")\n","            continue\n","\n","        # prepare label file content\n","        label_lines = []\n","        for contour in contours:\n","            if len(contour) < 3:\n","                continue  # not enough points to form a polygon\n","\n","            # bounding box\n","            x, y, w, h = cv2.boundingRect(contour)\n","\n","            # reduce bounding box dimensions by scaling down width and height\n","            x += int(w * 0.25)\n","            y += int(h * 0.25)\n","            w = int(w * 0.5)\n","            h = int(h * 0.5)\n","            x_center = (x + w / 2) / width\n","            y_center = (y + h / 2) / height\n","            bbox_width = (w / width)*0.5\n","            bbox_height = (h / height)*0.5\n","\n","            # segmentation polygon\n","            contour = contour.flatten()\n","            if len(contour) < 6:\n","                continue  # skip small contours\n","\n","            segmentation = []\n","            for i in range(0, len(contour), 2):\n","                x_point = contour[i] / width\n","                y_point = contour[i + 1] / height\n","                segmentation.append(f\"{x_point:.6f}\")\n","                segmentation.append(f\"{y_point:.6f}\")\n","            segmentation_str = ' '.join(segmentation)\n","\n","            # class ID (assuming 'eye' class is 0)\n","            class_id = 0\n","\n","            # combine into label line\n","            label_line = f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f} {segmentation_str}\"\n","            label_lines.append(label_line)\n","\n","        if not label_lines:\n","            print(f\"No valid segmentation found for {image_file}, skipping.\")\n","            continue\n","\n","        # Write to label file\n","        with open(label_path, 'w') as f:\n","            f.write('\\n'.join(label_lines))\n","\n","        print(f\"Processed {image_file}\")\n","\n","# paths to labels directories\n","train_labels_path = '/content/drive/MyDrive/dataset/train/labels'\n","val_labels_path = '/content/drive/MyDrive/dataset/val/labels'\n","\n","# convert training masks\n","convert_masks_to_yolo_format(train_images_path, train_masks_path, train_labels_path)\n","\n","# convert validation masks\n","convert_masks_to_yolo_format(val_images_path, val_masks_path, val_labels_path)"],"metadata":{"id":"63P4noAA3x42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create data.yaml file\n","data_yaml_content = f\"\"\"\n","path: /content/drive/MyDrive/dataset\n","\n","train: train/images\n","val: val/images\n","\n","# Number of classes\n","nc: 3\n","\n","\n","# Class names\n","names: ['eye', 'colorcard', 'bluecircle']\n","\"\"\"\n","\n","data_yaml_path = '/content/drive/MyDrive/dataset/data.yaml'\n","with open(data_yaml_path, 'w') as f:\n","    f.write(data_yaml_content)\n","\n","# display the data.yaml content\n","print(\"\\nCreated data.yaml file:\")\n","with open(data_yaml_path, 'r') as f:\n","    print(f.read())\n"],"metadata":{"id":"bhKEvy0c35Yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the YOLOv8 segmentation model\n","# load the YOLOv8 segmentation model\n","model = YOLO('yolov8x-seg.pt')\n","\n","# Train the model\n","model.train(\n","    data=data_yaml_path,\n","    epochs=50,\n","    imgsz=640,\n","    batch=16,\n","    name='eye_segmentation',\n","    task='segment',\n","    save = True\n",")"],"metadata":{"id":"dB3M7bct4LeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.val(data='/content/drive/MyDrive/dataset/data.yaml', imgsz=1024)"],"metadata":{"id":"eBEG5mGP38LZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = model.predict(source='/content/drive/MyDrive/dataset/test_images/1709617535926.jpg', save=True, imgsz=640)"],"metadata":{"id":"K9vKEjZh4BTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","display(Image(filename='runs/segment/train3/1709617535926.jpg'))"],"metadata":{"id":"5_TVjy1k4EEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run inference on test images\n","test_images_path = '/content/drive/MyDrive/dataset/test_images'\n","output_folder = '/content/drive/MyDrive/dataset/Predictions241201_x'\n","\n","# create output folder if it doesn't exist\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# run inference on all images in the test images folder\n","results = model.predict(\n","    source=test_images_path,\n","    save=True,\n","    conf=0.65,\n","    imgsz=640,\n","    task='segment'\n",")\n","\n","# move the predicted images to the output folder\n","for file_name in os.listdir(model.predictor.save_dir):\n","    if file_name.endswith(('.jpg', '.jpeg', '.png')):\n","        src_path = os.path.join(model.predictor.save_dir, file_name)\n","        dst_path = os.path.join(output_folder, file_name)\n","        shutil.move(src_path, dst_path)\n","\n","print(f\"Predictions saved to: {output_folder}\")"],"metadata":{"id":"XTphfeqQ4WQX"},"execution_count":null,"outputs":[]}]}